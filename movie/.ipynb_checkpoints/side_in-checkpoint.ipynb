{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6550dca5-03f9-42e1-a5f1-bc614834a248",
   "metadata": {},
   "source": [
    "# '''dahyun+darwin= dahwin'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a52c9b-36e9-43ef-9610-dffc9ab82340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips,vfx , AudioFileClip\n",
    "from moviepy import editor\n",
    "from moviepy.editor import transfx\n",
    "x = dir(transfx)\n",
    "clip1 = VideoFileClip('fancy.mp4').subclip(0,6).fx(vfx.fadein,1).fx(vfx.fadeout,1)\n",
    "clip2 = VideoFileClip('feel.mp4').subclip(0,6).fx(vfx.fadein,1).fx(vfx.fadeout,1)\n",
    "\n",
    "combined = concatenate_videoclips([clip1,clip2])\n",
    "combined.write_videofile('dahyun.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a52b40d-3056-4aeb-a677-a7739c11c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video test.mp4.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageClip, concatenate_videoclips\n",
    "\n",
    "# Step 1: Set the image filenames\n",
    "image_filenames = [\"nn.jpg\", \"n.jpeg\"]\n",
    "\n",
    "# Step 2: Set the duration for each image (in seconds)\n",
    "image_durations = [5, 5]  # Adjust the durations as needed\n",
    "\n",
    "# Step 3: Create ImageClip objects from the images\n",
    "image_clips = []\n",
    "for filename, duration in zip(image_filenames, image_durations):\n",
    "    image = ImageClip(filename, duration=duration)\n",
    "    image_clips.append(image)\n",
    "\n",
    "# Step 4: Concatenate the image clips into a single video clip\n",
    "video_clip = concatenate_videoclips(image_clips, method=\"chain\")\n",
    "\n",
    "# Step 5: Set the output video filename\n",
    "output_filename = \"test.mp4\"\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combined = video_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "# Step 6: Write the video file\n",
    "combined.write_videofile(output_filename, fps=10, codec=\"libx264\",\n",
    "                           audio_codec=\"aac\", bitrate=\"5000k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d8394-a571-40f1-9937-404496d1a48e",
   "metadata": {},
   "source": [
    "# Side in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90eb568d-5a8e-40a6-b11a-d85c1ee71824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, vfx\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "clip1 = VideoFileClip('likey.mp4').subclip(0, 6)\n",
    "clip2 = VideoFileClip('fancy.mp4').subclip(0, 6)\n",
    "\n",
    "duration = 30  # in frames\n",
    "\n",
    "frames1 = [frame for frame in clip1.iter_frames()][:-duration]\n",
    "duration_frames = [frame for frame in clip1.iter_frames()][-duration:]\n",
    "\n",
    "frames2 = [frame for frame in clip2.iter_frames()]\n",
    "\n",
    "height, width, _ = frames1[0].shape\n",
    "transition_frames = []\n",
    "\n",
    "for i in range(duration):\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    # For slide-in effect from right, clip1 moves to the left and clip2 comes from the right\n",
    "    tx = int((width / duration) * i)\n",
    "    tx_clip1 = -tx\n",
    "    tx_clip2 = width - tx\n",
    "\n",
    "    # Create a blank frame\n",
    "    blank_frame = np.zeros_like(frames1[0])\n",
    "\n",
    "    # # Convert duration frame from RGB to BGR\n",
    "    # frame_clip1 = cv2.cvtColor(duration_frames[i], cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(duration_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Apply translation to clip2\n",
    "    frame_clip2 = cv2.warpAffine(frames2[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    alpha = i / duration  # Linearly increasing alpha value\n",
    "\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "remaining_frames = frames2[duration:]\n",
    "\n",
    "final_frames = frames1 + transition_frames + remaining_frames\n",
    "fps = clip1.fps\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "for frame in final_frames:\n",
    "    # Convert frame from RGB to BGR\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    output.write(frame)\n",
    "\n",
    "output.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cc98e-c55d-4fde-a605-42f778644b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:  41%|███████████████████████████▏                                       | 255/629 [00:21<00:13, 28.53it/s, now=None]\u001b[A"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 2  # Duration of the slide-in transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-2, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 2)\n",
    "\n",
    "# Step 5: Convert the clips to numpy arrays\n",
    "clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-in transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / (transition_duration * clip1_last_second.fps)  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    tx = int((width / (transition_duration * clip1_last_second.fps)) * i)\n",
    "    tx_clip1 = -tx\n",
    "    tx_clip2 = width - tx\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "final_clip = concatenate_videoclips([video_clips[0], VideoClip(lambda t: transition_frames[int(t * clip1_last_second.fps)], duration=transition_duration), video_clips[1]])\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8ea60c-5293-41e5-a765-4e9747049f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "MoviePy - Writing audio in lastslideinTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-1, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 1)\n",
    "\n",
    "# Step 5: Convert the clips to numpy arrays\n",
    "clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-in transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / num_frames  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    tx = int((width / num_frames) * i)\n",
    "    tx_clip1 = -tx\n",
    "    tx_clip2 = width - tx\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), transition_clip.set_duration(transition_duration), video_clips[1].subclip(1, None)],method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d999b-a54e-45c1-ad5a-0551dfaf3a90",
   "metadata": {},
   "source": [
    "# center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d374526-e867-474c-96b5-82fe773c7eaa",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0750ae2d-a310-48c2-8684-14c4a91a00c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "MoviePy - Writing audio in lastslideinTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "def slidein(video0,video1):\n",
    "        # Step 3: Define transition duration\n",
    "    transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "    \n",
    "    # Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "    clip1_last_second = video0.subclip(-1, None)\n",
    "    clip2_first_second = video1.subclip(0, 1.3)\n",
    "    \n",
    "    # Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "    if clip1_last_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip1_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "        # clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    else:\n",
    "        clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    \n",
    "    # Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "    if clip2_first_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip2_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "        # clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    else:\n",
    "        \n",
    "        clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    \n",
    "    # Step 6: Perform slide-in transition using numpy and cv2\n",
    "    transition_frames = []\n",
    "    height, width, _ = clip1_frames[0].shape\n",
    "    \n",
    "    # Ensure the number of frames matches the expected duration\n",
    "    num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        alpha = i / num_frames  # Linearly increasing alpha value\n",
    "    \n",
    "        # Calculate the translation distance based on the frame index\n",
    "        tx_clip1 = -int((width / num_frames) * i)\n",
    "        tx_clip2 = width - int((width / num_frames) * i)\n",
    "    \n",
    "        # Apply translation to clip1\n",
    "        frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Handle the case where clip2_frames has fewer frames than expected\n",
    "        if i < len(clip2_frames):\n",
    "            # Apply translation to clip2\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "        else:\n",
    "            # Use the last frame of clip2_frames for the remaining frames\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Blend the frames to create the transition effect\n",
    "        blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "    \n",
    "        transition_frames.append(blend_frame)\n",
    "    \n",
    "    # Add remaining frames where clip2_first_second fills the entire screen\n",
    "    remaining_frames = clip2_frames[num_frames:]\n",
    "    transition_frames.extend(remaining_frames)\n",
    "    \n",
    "    \n",
    "    # Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "    transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "    transition_clip = transition_clip.set_duration(transition_duration)\n",
    "    return transition_clip\n",
    "    \n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, None)], method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e85849-4e9f-407a-b772-9655694041ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "MoviePy - Writing audio in lastslideinTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-1, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 1.3)\n",
    "\n",
    "# Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "if clip1_last_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip1_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "    # clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "else:\n",
    "    clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "\n",
    "# Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "if clip2_first_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip2_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "    # clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "else:\n",
    "    \n",
    "    clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-in transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / num_frames  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    tx_clip1 = -int((width / num_frames) * i)\n",
    "    tx_clip2 = width - int((width / num_frames) * i)\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Add remaining frames where clip2_first_second fills the entire screen\n",
    "remaining_frames = clip2_frames[num_frames:]\n",
    "transition_frames.extend(remaining_frames)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), transition_clip.set_duration(transition_duration), video_clips[1].subclip(1.3, None)], method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf162210-10ba-4f28-8f2d-b55b1f6c84b8",
   "metadata": {},
   "source": [
    "# Down to UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4830b90e-9a8f-4005-87d1-ca16f368d8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video slidedowntoup.mp4.\n",
      "MoviePy - Writing audio in slidedowntoupTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video slidedowntoup.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready slidedowntoup.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 1  # Duration of the slide-down-to-up transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-1, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 1)\n",
    "\n",
    "# Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "if clip1_last_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip1_frames = [cv2.copyMakeBorder(frame, padding, padding, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "else:\n",
    "    clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "\n",
    "# Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "if clip2_first_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip2_frames = [cv2.copyMakeBorder(frame, padding, padding, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "else:\n",
    "    clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-down-to-up transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / num_frames  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    ty_clip1 = int((height / num_frames) * i)\n",
    "    ty_clip2 = height - int((height / num_frames) * i)\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, 0], [0, 1, ty_clip1]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, 0], [0, 1, ty_clip2]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, 0], [0, 1, ty_clip2]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Add remaining frames where clip2_first_second fills the entire screen\n",
    "remaining_frames = clip2_frames[num_frames:]\n",
    "transition_frames.extend(remaining_frames)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), transition_clip.set_duration(transition_duration), video_clips[1].subclip(1, None)], method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"slidedowntoup.mp4\", codec=\"libx264\", fps=final_clip.fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b355e3eb-b6c3-4149-ab64-9f49d3751997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video 3_final_clip.mp4.\n",
      "Moviepy - Writing video 3_final_clip.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready 3_final_clip.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageClip, CompositeVideoClip\n",
    "from moviepy.editor import transfx\n",
    "\n",
    "EFFECT_DURATION = 0.3\n",
    "CLIP_DURATION = 7\n",
    "\n",
    "clip1 = ImageClip(\"m.jpeg\").set_duration(CLIP_DURATION)\n",
    "clip2 = ImageClip(\"n.jpeg\").set_duration(CLIP_DURATION)\n",
    "clip3 = ImageClip(\"nn.jpg\").set_duration(CLIP_DURATION)\n",
    "\n",
    "clips = [clip1, clip2, clip3]\n",
    "num_clips = len(clips)\n",
    "\n",
    "# Calculate the target width and height while maintaining aspect ratio\n",
    "target_width = 1920\n",
    "target_height = int(target_width * clip1.size[1] / clip1.size[0])\n",
    "\n",
    "# Create a list to store the final clips\n",
    "final_clips = []\n",
    "\n",
    "for i, clip in enumerate(clips):\n",
    "    start_time = (CLIP_DURATION - EFFECT_DURATION) * i\n",
    "\n",
    "    # Apply slide out effect to the current clip\n",
    "    slide_out_clip = clip.set_position(\"center\").fx(transfx.slide_out, duration=EFFECT_DURATION, side=\"left\")\n",
    "    slide_out_clip = slide_out_clip.set_start(start_time)\n",
    "    slide_out_clip = slide_out_clip.resize(width=target_width, height=target_height)\n",
    "\n",
    "    final_clips.append(slide_out_clip)\n",
    "\n",
    "    if i < num_clips - 1:\n",
    "        # Apply slide in effect to the next clip\n",
    "        slide_in_clip = clips[i+1].set_position(\"center\").fx(transfx.slide_in, duration=EFFECT_DURATION, side=\"right\")\n",
    "        slide_in_clip = slide_in_clip.set_start(start_time + EFFECT_DURATION)\n",
    "        slide_in_clip = slide_in_clip.resize(width=target_width, height=target_height)\n",
    "\n",
    "        final_clips.append(slide_in_clip)\n",
    "\n",
    "# Concatenate the final clips into a single video clip\n",
    "final_video_clip = CompositeVideoClip(final_clips, size=(target_width, target_height))\n",
    "\n",
    "# Write the video file\n",
    "final_video_clip.write_videofile(\"3_final_clip.mp4\", fps=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9eb65b5-2640-45c8-8444-9c36a26c5160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 1918\n",
      "Width: 1080\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Import the video\n",
    "cap = cv2.VideoCapture(\"motion.mp4\")\n",
    "\n",
    "# Get the height and width of the video\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# Print the height and width of the video\n",
    "print(\"Height:\", height)\n",
    "print(\"Width:\", width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6913133c-4ffb-491f-9149-f0dce5f9e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 1080.0\n",
      "Width: 1920.0\n",
      "Aspect Ratio: 1.78:1.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = r\"D:\\video\\Video\\Video\\twice\\twice.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the width and height of the video\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(\"Height:\", height)\n",
    "print(\"Width:\", width)\n",
    "# Calculate the aspect ratio\n",
    "aspect_ratio = width / height\n",
    "\n",
    "print(\"Aspect Ratio: {:.2f}:{:.2f}\".format(aspect_ratio, 1))\n",
    "\n",
    "# Release the video capture\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d16a15-d3eb-48cf-8e03-1de801dc704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "\n",
    "# Load the video clips\n",
    "clip1 = VideoFileClip('fancy.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "clip2 = VideoFileClip('motion.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "\n",
    "# Resize the clips to match the aspect ratio of the first clip (fancy.mp4)\n",
    "clip2 = clip2.resize(height=clip1.size[1])\n",
    "\n",
    "# Concatenate the clips\n",
    "combined = concatenate_videoclips([clip1, clip2])\n",
    "\n",
    "# Write the final video file\n",
    "combined.write_videofile('new.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99352208-8f25-441c-957e-987799f856b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "\n",
    "# Set the desired aspect ratio\n",
    "desired_aspect_ratio = 16 / 9  # For example, 16:9\n",
    "\n",
    "# Load the video clips\n",
    "clip1 = VideoFileClip('fancy.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "clip2 = VideoFileClip('1686498673.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "\n",
    "# Calculate the current aspect ratios of the clips\n",
    "aspect_ratio1 = clip1.size[0] / clip1.size[1]\n",
    "aspect_ratio2 = clip2.size[0] / clip2.size[1]\n",
    "\n",
    "# Resize the clips to match the desired aspect ratio\n",
    "clip1_resized = clip1.resize(height=int(clip1.size[0] / desired_aspect_ratio))\n",
    "clip2_resized = clip2.resize(height=int(clip2.size[0] / desired_aspect_ratio))\n",
    "\n",
    "# Concatenate the resized clips\n",
    "final_clip = concatenate_videoclips([clip1_resized, clip2_resized])\n",
    "\n",
    "# Write the final video file\n",
    "final_clip.write_videofile('new.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49b814-443b-4ddb-b4e9-4b4e0b67f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def export_black_video(filename, width, height, duration):\n",
    "  \"\"\"Exports a full black video to the specified filename.\n",
    "\n",
    "  Args:\n",
    "    filename: The filename of the video to export.\n",
    "    width: The width of the video.\n",
    "    height: The height of the video.\n",
    "    duration: The duration of the video in seconds.\n",
    "  \"\"\"\n",
    "\n",
    "  video = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))\n",
    "  black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "  for i in range(duration * 30):\n",
    "    video.write(black_frame)\n",
    "  video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  export_black_video('black.mp4', 1280, 720, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0d55e-cdeb-4333-bda9-7512e35a020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.video.fx.resize import resize\n",
    "\n",
    "# Load the video clip\n",
    "clip = VideoFileClip(\"motion.mp4\").subclip(0, 6)\n",
    "\n",
    "# Calculate the desired height based on the 16:9 aspect ratio\n",
    "desired_height = int(clip.w * 9 / 16)\n",
    "\n",
    "# Resize the clip while maintaining the aspect ratio\n",
    "resized_clip = resize(clip, height=desired_height)\n",
    "\n",
    "# Write the resized clip to a new file\n",
    "resized_clip.write_videofile(\"motion_resized.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7dfa4-fe09-41fd-b110-c75be9d1c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "from moviepy.video.fx.resize import resize\n",
    "\n",
    "# Load the video clips\n",
    "clip_fancy = VideoFileClip(\"fancy.mp4\").subclip(0, 6)\n",
    "# Load the video clip\n",
    "clip = VideoFileClip(\"motion.mp4\").subclip(0, 6)\n",
    "\n",
    "# Calculate the desired height based on the 16:9 aspect ratio\n",
    "desired_height = int(clip.w * 9 / 16)\n",
    "\n",
    "# Resize the clip while maintaining the aspect ratio\n",
    "resized_clip = resize(clip, height=desired_height)\n",
    "# Concatenate the clips\n",
    "combined_clip = concatenate_videoclips([clip_fancy, resized_clip])\n",
    "\n",
    "# Write the combined clip to a new file\n",
    "combined_clip.write_videofile(\"combined.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b9bb6-7490-4f2f-99f6-9e12a4df3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "clip = VideoFileClip(\"motion.mp4\").subclip(0,6)\n",
    "\n",
    "combine = clips_array([[clip]])\n",
    "cambiado = combine.fx(vfx.resize,(1920,1080),width= 1980)\n",
    "cambiado.write_videofile(\"combina5v.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fcaef-7639-4f87-8d7a-5eb7221cfde7",
   "metadata": {},
   "source": [
    "# PERFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59696cd5-94fd-4356-aeec-42d4a57dbe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "600\n",
      "Moviepy - Building video motion_on_black_background.mp4.\n",
      "MoviePy - Writing audio in motion_on_black_backgroundTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video motion_on_black_background.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready motion_on_black_background.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "# Load the motion clip\n",
    "clip_motion = VideoFileClip(r\"fancy.mp4\").subclip(0, 6)\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1920\n",
    "\n",
    "# Create a black color clip as the background\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(6)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = (background_width - clip_motion.w) // 2\n",
    "y_pos = (background_height - clip_motion.h) // 2\n",
    "print(x_pos)\n",
    "print(y_pos)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "final_clip.write_videofile(\"motion_on_black_background.mp4\", fps=clip_motion.fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625d00e1-617f-4173-b08c-164a96bdb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "600\n",
      "Moviepy - Building video last.mp4.\n",
      "MoviePy - Writing audio in lastTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video last.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready last.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "# Load the motion clip\n",
    "clip_motion = VideoFileClip(r\"fancy.mp4\").subclip(0, 6)\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1920\n",
    "\n",
    "# Create a black color clip as the background\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(6)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = (background_width - clip_motion.w) // 2\n",
    "y_pos = (background_height - clip_motion.h) // 2\n",
    "print(x_pos)\n",
    "print(y_pos)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combine = final_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "combine.write_videofile(\"last.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a78d0af1-a2a7-4f36-85b4-653393f6b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video my_reels.mp4.\n",
      "MoviePy - Writing audio in my_reelsTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video my_reels.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready my_reels.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1980\n",
    "\n",
    "# Create a black color clip as the background\n",
    "\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(10)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = 0\n",
    "y_pos = 1\n",
    "clip_motion = VideoFileClip(r\"motion.mp4\").subclip(0, 6)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "final_clip.write_videofile(\"my_reels.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d91edc14-b1eb-46c5-bfa7-5329a728c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video my_reels.mp4.\n",
      "MoviePy - Writing audio in my_reelsTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video my_reels.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready my_reels.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1980\n",
    "\n",
    "# Create a black color clip as the background\n",
    "\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(10)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = 0\n",
    "y_pos = 1\n",
    "clip_motion = VideoFileClip(r\"fancy.mp4\").subclip(0, 6)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "final_clip.write_videofile(\"my_reels.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57a855-a6a4-4b50-80c8-3c929b1e7052",
   "metadata": {},
   "source": [
    "# Multiple screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c71d6-c82c-4b77-a669-e279d65b5f62",
   "metadata": {},
   "source": [
    "# Don't use 4k, it's so slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0327c97-1110-4e31-a438-4f566c9770cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video combined1.mp4.\n",
      "MoviePy - Writing audio in combined1TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video combined1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready combined1.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "clip1 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp4\").subclip(87, 93).margin(8)\n",
    "clip2 = VideoFileClip(\"fancy.mp4\").subclip(0, 6).margin(8)\n",
    "clip3 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp4\").subclip(87, 93).margin(8)\n",
    "clip4 = VideoFileClip(\"fancy.mp4\").subclip(0, 6).margin(8)\n",
    "clip2.resize((1920,1080))\n",
    "clip4.resize((1920,1080))\n",
    "combine = clips_array([[clip1, clip2],\n",
    "                       [clip3, clip4]])\n",
    "\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combine = combine.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "\n",
    "# Write the combined clips to a new file\n",
    "combine.write_videofile('combined1.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d3be3-b5c8-4811-a4d1-b94fb01c394b",
   "metadata": {},
   "source": [
    "# Perfect export of different resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1ec1fb-ee1b-4cd8-8cf1-f41d30bb7564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video dahyun0.mp4.\n",
      "MoviePy - Writing audio in dahyun0TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video dahyun0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready dahyun0.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "from moviepy.editor import *\n",
    "# clip1 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp\").subclip(87, 93)\n",
    "clip1 = VideoFileClip(\"feel.mp4\",target_resolution=(1080, 1920)).subclip(0,6)\n",
    "clip2 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp4\").subclip(0,6)\n",
    "\n",
    "combine = concatenate_videoclips([clip1,clip2],method=\"compose\")\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combined = combine.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "combined.write_videofile('dahyun0.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5dc9a7-e7b2-4721-96ed-bb47e064f365",
   "metadata": {},
   "source": [
    "# traget resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499b93ba-5e24-429d-a806-a155c5ac5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"combined.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define the target resolution\n",
    "target_width = 1080\n",
    "target_height = 1920\n",
    "\n",
    "# Get the original video's width and height\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "# Calculate the aspect ratio of the original video\n",
    "aspect_ratio = width / height\n",
    "\n",
    "# Calculate the new dimensions for resizing\n",
    "if aspect_ratio > target_width / target_height:\n",
    "    new_width = target_width\n",
    "    new_height = int(target_width / aspect_ratio)\n",
    "else:\n",
    "    new_height = target_height\n",
    "    new_width = int(target_height * aspect_ratio)\n",
    "\n",
    "# Create an output video writer\n",
    "output_path = \"output_combined_hd.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "\n",
    "# Process each frame of the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to the target dimensions\n",
    "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    # Write the resized frame to the output video\n",
    "    out.write(resized_frame)\n",
    "\n",
    "    # Display the resized frame\n",
    "    cv2.imshow(\"Resized Frame\", resized_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and output video writer\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d0e8069-8905-4d02-8320-ee080616cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # End of the video\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Reels Video\", frame)\n",
    "\n",
    "    # Check for the 'q' key to quit\n",
    "    if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af55bb49-c522-4268-aca2-91ad9637ee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of active threads: 7\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "# Get the number of active threads\n",
    "num_threads = threading.active_count()\n",
    "\n",
    "print(\"Number of active threads:\", num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5e1a-7482-439b-b7b7-10562f972965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
