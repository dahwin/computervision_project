{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6550dca5-03f9-42e1-a5f1-bc614834a248",
   "metadata": {},
   "source": [
    "# '''dahyun+darwin= dahwin'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a52c9b-36e9-43ef-9610-dffc9ab82340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips,vfx , AudioFileClip\n",
    "from moviepy import editor\n",
    "from moviepy.editor import transfx\n",
    "x = dir(transfx)\n",
    "clip1 = VideoFileClip('fancy.mp4').subclip(0,6).fx(vfx.fadein,1).fx(vfx.fadeout,1)\n",
    "clip2 = VideoFileClip('feel.mp4').subclip(0,6).fx(vfx.fadein,1).fx(vfx.fadeout,1)\n",
    "\n",
    "combined = concatenate_videoclips([clip1,clip2])\n",
    "combined.write_videofile('dahyun.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a52b40d-3056-4aeb-a677-a7739c11c501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video test.mp4.\n",
      "Moviepy - Writing video test.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageClip, concatenate_videoclips\n",
    "\n",
    "# Step 1: Set the image filenames\n",
    "image_filenames = [\"nn.jpg\", \"n.jpeg\"]\n",
    "\n",
    "# Step 2: Set the duration for each image (in seconds)\n",
    "image_durations = [5, 5]  # Adjust the durations as needed\n",
    "\n",
    "# Step 3: Create ImageClip objects from the images\n",
    "image_clips = []\n",
    "for filename, duration in zip(image_filenames, image_durations):\n",
    "    image = ImageClip(filename, duration=duration)\n",
    "    image_clips.append(image)\n",
    "\n",
    "# Step 4: Concatenate the image clips into a single video clip\n",
    "video_clip = concatenate_videoclips(image_clips, method=\"chain\")\n",
    "\n",
    "# Step 5: Set the output video filename\n",
    "output_filename = \"test.mp4\"\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combined = video_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "# Step 6: Write the video file\n",
    "combined.write_videofile(output_filename, fps=10, codec=\"libx264\",\n",
    "                           audio_codec=\"aac\", bitrate=\"5000k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d8394-a571-40f1-9937-404496d1a48e",
   "metadata": {},
   "source": [
    "# Side in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90eb568d-5a8e-40a6-b11a-d85c1ee71824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, vfx\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "clip1 = VideoFileClip('likey.mp4').subclip(0, 6)\n",
    "clip2 = VideoFileClip('fancy.mp4').subclip(0, 6)\n",
    "\n",
    "duration = 30  # in frames\n",
    "\n",
    "frames1 = [frame for frame in clip1.iter_frames()][:-duration]\n",
    "duration_frames = [frame for frame in clip1.iter_frames()][-duration:]\n",
    "\n",
    "frames2 = [frame for frame in clip2.iter_frames()]\n",
    "\n",
    "height, width, _ = frames1[0].shape\n",
    "transition_frames = []\n",
    "\n",
    "for i in range(duration):\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    # For slide-in effect from right, clip1 moves to the left and clip2 comes from the right\n",
    "    tx = int((width / duration) * i)\n",
    "    tx_clip1 = -tx\n",
    "    tx_clip2 = width - tx\n",
    "\n",
    "    # Create a blank frame\n",
    "    blank_frame = np.zeros_like(frames1[0])\n",
    "\n",
    "    # # Convert duration frame from RGB to BGR\n",
    "    # frame_clip1 = cv2.cvtColor(duration_frames[i], cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(duration_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Apply translation to clip2\n",
    "    frame_clip2 = cv2.warpAffine(frames2[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    alpha = i / duration  # Linearly increasing alpha value\n",
    "\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "remaining_frames = frames2[duration:]\n",
    "\n",
    "final_frames = frames1 + transition_frames + remaining_frames\n",
    "fps = clip1.fps\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "for frame in final_frames:\n",
    "    # Convert frame from RGB to BGR\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    output.write(frame)\n",
    "\n",
    "output.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cc98e-c55d-4fde-a605-42f778644b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:  41%|███████████████████████████▏                                       | 255/629 [00:21<00:13, 28.53it/s, now=None]\u001b[A"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 2  # Duration of the slide-in transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-2, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 2)\n",
    "\n",
    "# Step 5: Convert the clips to numpy arrays\n",
    "clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-in transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / (transition_duration * clip1_last_second.fps)  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    tx = int((width / (transition_duration * clip1_last_second.fps)) * i)\n",
    "    tx_clip1 = -tx\n",
    "    tx_clip2 = width - tx\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "final_clip = concatenate_videoclips([video_clips[0], VideoClip(lambda t: transition_frames[int(t * clip1_last_second.fps)], duration=transition_duration), video_clips[1]])\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8ea60c-5293-41e5-a765-4e9747049f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "MoviePy - Writing audio in lastslideinTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-1, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 1)\n",
    "\n",
    "# Step 5: Convert the clips to numpy arrays\n",
    "clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-in transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / num_frames  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    tx = int((width / num_frames) * i)\n",
    "    tx_clip1 = -tx\n",
    "    tx_clip2 = width - tx\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), transition_clip.set_duration(transition_duration), video_clips[1].subclip(1, None)],method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ee93871-ff0a-4bd1-bd8e-fa64c482ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1), video_clips[1].subclip(0, video_clips[1].duration - 1), slidein(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')\n",
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1), video_clips[1].subclip(0, video_clips[1].duration - 1), fadedown(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, video_clips[2].duration - 1), video_clips[2].subclip(0, video_clips[2].duration - 1), fadein(video_clips[2], video_clips[3]), video_clips[3].subclip(1.3, video_clips[3].duration - 1), video_clips[3].subclip(0, video_clips[3].duration - 1), fun(video_clips[3], video_clips[4]), video_clips[4].subclip(1.3, video_clips[4].duration - 1), video_clips[4].subclip(0, video_clips[4].duration - 1), funck(video_clips[4], video_clips[5]), video_clips[5].subclip(1.3, None)], method='compose')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_string_to_final_clip(input_string):\n",
    "    # Define regular expression patterns to match video_clips[i] and custom function expressions\n",
    "    video_clip_pattern = r'video_clips\\[(\\d+)\\]'\n",
    "    custom_function_pattern = r'(\\w+)\\(\\)'\n",
    "\n",
    "    # Find all occurrences of video_clips[i] and custom function expressions in the input string\n",
    "    video_clip_matches = re.findall(video_clip_pattern, input_string)\n",
    "    custom_function_matches = re.findall(custom_function_pattern, input_string)\n",
    "\n",
    "    # Check if the input string contains at least one video_clips[i] and one custom function expression\n",
    "    if not video_clip_matches or not custom_function_matches:\n",
    "        return \"Invalid input string. It should contain at least one video_clips[i] and one custom function expression.\"\n",
    "\n",
    "    # Construct the final_clip string\n",
    "    final_clip_str = \"concatenate_videoclips([\"\n",
    "\n",
    "    num_clips = min(len(video_clip_matches), len(custom_function_matches))\n",
    "    for i in range(num_clips):\n",
    "        video_idx = int(video_clip_matches[i])\n",
    "        custom_function_name = custom_function_matches[i]\n",
    "\n",
    "        custom_function_str = \"{}(video_clips[{}], video_clips[{}])\".format(custom_function_name, video_idx, video_idx + 1)\n",
    "\n",
    "        subclip_str1 = \"video_clips[{}].subclip(0, video_clips[{}].duration - 1)\".format(video_idx, video_idx)\n",
    "\n",
    "        if i == num_clips - 1:  # Last video clip\n",
    "            subclip_str2 = \"video_clips[{}].subclip(1.3, None)\".format(video_idx + 1)\n",
    "        else:\n",
    "            subclip_str2 = \"video_clips[{}].subclip(1.3, video_clips[{}].duration - 1)\".format(video_idx + 1, video_idx + 1)\n",
    "\n",
    "        final_clip_str += \"{}, {}, {}, \".format(subclip_str1, custom_function_str, subclip_str2)\n",
    "\n",
    "    # Remove the last two characters \", \" from the final_clip_str\n",
    "    final_clip_str = final_clip_str[:-2]\n",
    "\n",
    "    # Add the method argument and closing bracket\n",
    "    final_clip_str += \"], method='compose')\"\n",
    "\n",
    "    return final_clip_str\n",
    "\n",
    "# Test examples\n",
    "input_string1 = 'video_clips[0]+slidein()+video_clips[1]+slidein()+video_clips[2]'\n",
    "input_string2 = 'video_clips[0]+slidein()+video_clips[1]+fadedown()+video_clips[2]+fadein()+video_clips[3]+fun()+video_clips[4]+funck()+video_clips[5]'\n",
    "\n",
    "print(convert_string_to_final_clip(input_string1))\n",
    "print(convert_string_to_final_clip(input_string2))\n",
    "# concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1),slidein(video_clips[1],video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f495d471-4ed8-47eb-a84b-6e92583b56df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1), slidein(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, video_clips[2].duration - 1), slidein(video_clips[2], video_clips[3]), video_clips[3].subclip(1.3, video_clips[3].duration - 1)], method='compose')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_string_to_final_clip(input_string):\n",
    "    # Define regular expression patterns to match video_clips[i] and custom function expressions\n",
    "    video_clip_pattern = r'video_clips\\[(\\d+)\\]'\n",
    "    custom_function_pattern = r'(\\w+)\\(\\)'\n",
    "\n",
    "    # Find all occurrences of video_clips[i] and custom function expressions in the input string\n",
    "    video_clip_matches = re.findall(video_clip_pattern, input_string)\n",
    "    custom_function_matches = re.findall(custom_function_pattern, input_string)\n",
    "\n",
    "    # Check if the input string contains at least one video_clips[i] and one custom function expression\n",
    "    if not video_clip_matches or not custom_function_matches:\n",
    "        return \"Invalid input string. It should contain at least one video_clips[i] and one custom function expression.\"\n",
    "\n",
    "    # Construct the final_clip string\n",
    "    final_clip_str = \"concatenate_videoclips([\"\n",
    "\n",
    "    num_clips = len(video_clip_matches)\n",
    "\n",
    "    for i, (video_idx, custom_function_name) in enumerate(zip(video_clip_matches, custom_function_matches)):\n",
    "        video_idx = int(video_idx)\n",
    "\n",
    "        custom_function_str = \"{}(video_clips[{}], video_clips[{}])\".format(custom_function_name, video_idx, video_idx + 1)\n",
    "\n",
    "        if i == 0:  # First video clip\n",
    "            subclip_str1 = \"video_clips[{}].subclip(0, video_clips[{}].duration - 1)\".format(video_idx, video_idx)\n",
    "        else:\n",
    "            subclip_str1 = \"video_clips[{}].subclip(1.3, video_clips[{}].duration - 1)\".format(video_idx, video_idx)\n",
    "\n",
    "        if i == num_clips - 1:  # Last video clip\n",
    "            subclip_str2 = \"video_clips[{}].subclip(1.3, None)\".format(video_idx)\n",
    "        else:\n",
    "            next_video_idx = int(video_clip_matches[i + 1])\n",
    "            subclip_str2 = \"video_clips[{}].subclip(1.3, video_clips[{}].duration - 1)\".format(video_idx + 1, next_video_idx)\n",
    "\n",
    "        final_clip_str += \"{}, {}, \".format(subclip_str1, custom_function_str)\n",
    "    \n",
    "    # Add the last video clip\n",
    "    final_clip_str += \"{}], method='compose')\".format(subclip_str2)\n",
    "\n",
    "    return final_clip_str\n",
    "\n",
    "# Test example\n",
    "input_string1 = 'video_clips[0]+slidein()+video_clips[1]+slidein()+video_clips[2]+slidein()+video_clips[3]'\n",
    "string = convert_string_to_final_clip(input_string1)\n",
    "print(string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9ebd2520-de68-4ce1-9a04-c26d904a9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_clips[3].subclip(1.3, video_clips[3].duration - 1)\n",
      "3\n",
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1), slidein(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, video_clips[2].duration - 1), slidein(video_clips[2], video_clips[3]), video_clips[3].subclip(1.3, None)], method='compose')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "video_clips_string = string\n",
    "# Use regular expression to extract the last part of the list\n",
    "pattern = r\"video_clips\\[\\d+\\]\\.subclip\\([^)]+\\)\"\n",
    "matches = re.findall(pattern, video_clips_string)\n",
    "\n",
    "if matches:\n",
    "    last_part_of_list = matches[-1]\n",
    "    print(last_part_of_list)\n",
    "else:\n",
    "    print(\"No match found.\")\n",
    "\n",
    "def get_number_from_string(string):\n",
    "\n",
    "  match = re.search(r\"\\d+\", string)\n",
    "  if match:\n",
    "    return int(match.group(0))\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "number = get_number_from_string(last_part_of_list)\n",
    "\n",
    "print(number)\n",
    "\n",
    "import re\n",
    "\n",
    "input_string = video_clips_string\n",
    "\n",
    "# Define the regular expression pattern to match the desired part\n",
    "pattern = f\"video_clips\\[{number}\\].subclip\\(1\\.3, video_clips\\[{number}\\].duration - 1\\)\"\n",
    "\n",
    "# Replace the matched part with the new desired part\n",
    "clips = re.sub(pattern, f\"video_clips[{number}].subclip(1.3, None)\", input_string)\n",
    "\n",
    "print(clips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d999b-a54e-45c1-ad5a-0551dfaf3a90",
   "metadata": {},
   "source": [
    "# center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d374526-e867-474c-96b5-82fe773c7eaa",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0750ae2d-a310-48c2-8684-14c4a91a00c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "MoviePy - Writing audio in lastslideinTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = ['fancy.mp4','resized_tofu.mp4','fancy.mp4','feel.mp4']\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "def slidein(video0,video1):\n",
    "        # Step 3: Define transition duration\n",
    "    transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "    \n",
    "    # Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "    clip1_last_second = video0.subclip(-1, None)\n",
    "    clip2_first_second = video1.subclip(0, 1.3)\n",
    "    \n",
    "    # Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "    if clip1_last_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip1_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "        # clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    else:\n",
    "        clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    \n",
    "    # Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "    if clip2_first_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip2_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "        # clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    else:\n",
    "        \n",
    "        clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    \n",
    "    # Step 6: Perform slide-in transition using numpy and cv2\n",
    "    transition_frames = []\n",
    "    height, width, _ = clip1_frames[0].shape\n",
    "    \n",
    "    # Ensure the number of frames matches the expected duration\n",
    "    num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        alpha = i / num_frames  # Linearly increasing alpha value\n",
    "    \n",
    "        # Calculate the translation distance based on the frame index\n",
    "        tx_clip1 = -int((width / num_frames) * i)\n",
    "        tx_clip2 = width - int((width / num_frames) * i)\n",
    "    \n",
    "        # Apply translation to clip1\n",
    "        frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Handle the case where clip2_frames has fewer frames than expected\n",
    "        if i < len(clip2_frames):\n",
    "            # Apply translation to clip2\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "        else:\n",
    "            # Use the last frame of clip2_frames for the remaining frames\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Blend the frames to create the transition effect\n",
    "        blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "    \n",
    "        transition_frames.append(blend_frame)\n",
    "    \n",
    "    # Add remaining frames where clip2_first_second fills the entire screen\n",
    "    remaining_frames = clip2_frames[num_frames:]\n",
    "    transition_frames.extend(remaining_frames)\n",
    "    \n",
    "    \n",
    "    # Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "    transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "    transition_clip = transition_clip.set_duration(transition_duration)\n",
    "    return transition_clip\n",
    "    \n",
    "# Use the convert_string_to_final_clip function to get the final_clip\n",
    "# final_clip_str = convert_string_to_final_clip(input_string1)\n",
    "\n",
    "# Evaluate the final_clip_str to get the final_clip video clip object\n",
    "final_clip = eval(clips)\n",
    "\n",
    "\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "final_clips = final_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "# Step 8: Write the final video to a file\n",
    "final_clips.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6df93d7-543b-491c-98e4-c9bb8859592a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Moviepy: ImageSequenceClip requires all images to be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 89\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transition_clip\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Use the convert_string_to_final_clip function to get the final_clip\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# final_clip_str = convert_string_to_final_clip(input_string1)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Evaluate the final_clip_str to get the final_clip video clip object\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m final_clip \u001b[38;5;241m=\u001b[39m final_clip \u001b[38;5;241m=\u001b[39m concatenate_videoclips([video_clips[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msubclip(\u001b[38;5;241m0\u001b[39m, video_clips[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[43mslidein\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_clips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvideo_clips\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, video_clips[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msubclip(\u001b[38;5;241m1.3\u001b[39m, video_clips[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),slidein(video_clips[\u001b[38;5;241m1\u001b[39m],video_clips[\u001b[38;5;241m2\u001b[39m]), video_clips[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39msubclip(\u001b[38;5;241m1.3\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)], method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     92\u001b[0m target_resolution \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1920\u001b[39m, \u001b[38;5;241m1080\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Resize the combined clip to the target resolution\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 81\u001b[0m, in \u001b[0;36mslidein\u001b[1;34m(video0, video1)\u001b[0m\n\u001b[0;32m     77\u001b[0m transition_frames\u001b[38;5;241m.\u001b[39mextend(remaining_frames)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Step 7: Concatenate the original clips and the transition frames to create the final video\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m transition_clip \u001b[38;5;241m=\u001b[39m \u001b[43mImageSequenceClip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip1_last_second\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m transition_clip \u001b[38;5;241m=\u001b[39m transition_clip\u001b[38;5;241m.\u001b[39mset_duration(transition_duration)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transition_clip\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\moviepy\\video\\io\\ImageSequenceClip.py:91\u001b[0m, in \u001b[0;36mImageSequenceClip.__init__\u001b[1;34m(self, sequence, fps, durations, with_mask, ismask, load_images)\u001b[0m\n\u001b[0;32m     89\u001b[0m        image1\u001b[38;5;241m=\u001b[39mimread(image)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m!=\u001b[39m image1\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m---> 91\u001b[0m        \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoviepy: ImageSequenceClip requires all images to be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfps \u001b[38;5;241m=\u001b[39m fps\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mException\u001b[0m: Moviepy: ImageSequenceClip requires all images to be the same size"
     ]
    }
   ],
   "source": [
    "# final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, None)], method='compose')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = ['people-in-a-conference-room-for-a-business-meeting_duration_21_second.mp4','a-monk-meditating-on-a-tree_duration_15_second.mp4','sweet-foods-causing-diabetes_duration_9_second.mp4','a-monk-meditating-on-a-tree_duration_15_second.mp4']\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "def slidein(video0,video1):\n",
    "        # Step 3: Define transition duration\n",
    "    transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "    \n",
    "    # Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "    clip1_last_second = video0.subclip(-1, None)\n",
    "    clip2_first_second = video1.subclip(0, 1.3)\n",
    "    \n",
    "    # Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "    if clip1_last_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip1_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "        # clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    else:\n",
    "        clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    \n",
    "    # Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "    if clip2_first_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip2_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "        # clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    else:\n",
    "        \n",
    "        clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    \n",
    "    # Step 6: Perform slide-in transition using numpy and cv2\n",
    "    transition_frames = []\n",
    "    height, width, _ = clip1_frames[0].shape\n",
    "    \n",
    "    # Ensure the number of frames matches the expected duration\n",
    "    num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        alpha = i / num_frames  # Linearly increasing alpha value\n",
    "    \n",
    "        # Calculate the translation distance based on the frame index\n",
    "        tx_clip1 = -int((width / num_frames) * i)\n",
    "        tx_clip2 = width - int((width / num_frames) * i)\n",
    "    \n",
    "        # Apply translation to clip1\n",
    "        frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Handle the case where clip2_frames has fewer frames than expected\n",
    "        if i < len(clip2_frames):\n",
    "            # Apply translation to clip2\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "        else:\n",
    "            # Use the last frame of clip2_frames for the remaining frames\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Blend the frames to create the transition effect\n",
    "        blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "    \n",
    "        transition_frames.append(blend_frame)\n",
    "    \n",
    "    # Add remaining frames where clip2_first_second fills the entire screen\n",
    "    remaining_frames = clip2_frames[num_frames:]\n",
    "    transition_frames.extend(remaining_frames)\n",
    "    \n",
    "    \n",
    "    # Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "    transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "    transition_clip = transition_clip.set_duration(transition_duration)\n",
    "    return transition_clip\n",
    "    \n",
    "# Use the convert_string_to_final_clip function to get the final_clip\n",
    "# final_clip_str = convert_string_to_final_clip(input_string1)\n",
    "\n",
    "# Evaluate the final_clip_str to get the final_clip video clip object\n",
    "final_clip = final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1),slidein(video_clips[1],video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')\n",
    "\n",
    "\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "final_clips = final_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "# Step 8: Write the final video to a file\n",
    "final_clips.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb3ea7b-75c5-4083-921a-970633aec7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = ['black-and-white-video-of-busy-street_duration_14_second.mp4','a-monk-meditating-on-a-tree_duration_15_second.mp4','sweet-foods-causing-diabetes_duration_9_second.mp4','a-monk-meditating-on-a-tree_duration_15_second.mp4']\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "def slidein(video0,video1):\n",
    "        # Step 3: Define transition duration\n",
    "    transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "    \n",
    "    # Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "    clip1_last_second = video0.subclip(-1, None)\n",
    "    clip2_first_second = video1.subclip(0, 1.3)\n",
    "    \n",
    "    # Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "    if clip1_last_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip1_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "        # clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    else:\n",
    "        clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "    \n",
    "    # Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "    if clip2_first_second.aspect_ratio < 1.78:\n",
    "        width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "        padding = (1920 - width) // 2\n",
    "        clip2_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "        # clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    else:\n",
    "        \n",
    "        clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "    \n",
    "    # Step 6: Perform slide-in transition using numpy and cv2\n",
    "    transition_frames = []\n",
    "    height, width, _ = clip1_frames[0].shape\n",
    "    \n",
    " # Ensure the number of frames matches the expected duration\n",
    "    num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        alpha = i / num_frames  # Linearly increasing alpha value\n",
    "    \n",
    "        # Calculate the translation distance based on the frame index\n",
    "        tx_clip1 = -int((width / num_frames) * i)\n",
    "        tx_clip2 = width - int((width / num_frames) * i)\n",
    "    \n",
    "        # Apply translation to clip1\n",
    "        frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Handle the case where clip2_frames has fewer frames than expected\n",
    "        if i < len(clip2_frames):\n",
    "            # Apply translation to clip2\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "        else:\n",
    "            # Use the last frame of clip2_frames for the remaining frames\n",
    "            frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    \n",
    "        # Blend the frames to create the transition effect\n",
    "        blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "    \n",
    "        transition_frames.append(blend_frame)\n",
    "    \n",
    "    # Add remaining frames where clip2_first_second fills the entire screen\n",
    "    remaining_frames = clip2_frames[num_frames:]\n",
    "    transition_frames.extend(remaining_frames)\n",
    "    \n",
    "     # Step 7: Create the transition clip using NumPy\n",
    "    transition_clip = VideoClip(make_frame=lambda t: transition_frames[int(t * clip1_last_second.fps)], duration=transition_duration)\n",
    "\n",
    "    return transition_clip\n",
    "\n",
    "    \n",
    "# Use the convert_string_to_final_clip function to get the final_clip\n",
    "# final_clip_str = convert_string_to_final_clip(input_string1)\n",
    "\n",
    "# Evaluate the final_clip_str to get the final_clip video clip object\n",
    "final_clip = final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1),slidein(video_clips[1],video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')\n",
    "\n",
    "\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "final_clips = final_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "# Step 8: Write the final video to a file\n",
    "final_clips.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "447ee40b-fb2b-477e-a121-3be023a473dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video resized_tofu.mp4.\n",
      "MoviePy - Writing audio in resized_tofuTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  56%|██████████████████████████████████▌                           | 236/423 [00:00<00:00, 855.56it/s, now=None]\n",
      "KeyboardInterrupt\n",
      "\n",
      "chunk:  60%|█████████████████████████████████████▏                        | 254/423 [00:17<00:00, 855.56it/s, now=None]"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filename\n",
    "video_filename = \"tofu.mp4\"\n",
    "\n",
    "# Step 2: Create a VideoFileClip object from the video\n",
    "video_clip = VideoFileClip(video_filename)\n",
    "\n",
    "\n",
    "# Step 4: Resize the video clip while maintaining the aspect ratio\n",
    "resized_clip = video_clip.resize(width=1080, height=1920)\n",
    "\n",
    "# Step 5: Write the resized video to a new file\n",
    "resized_clip.write_videofile(\"resized_tofu.mp4\", codec=\"libx264\", fps=resized_clip.fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "721bd3e5-4536-42d3-844d-a697f39b6496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 1080\n",
      "Width: 607\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Step 1: Set the video filename\n",
    "video_filename = \"tofu.mp4\"\n",
    "\n",
    "# Step 2: Create a VideoCapture object from the video\n",
    "cap = cv2.VideoCapture(video_filename)\n",
    "\n",
    "# Step 3: Get the height and width of the video\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# Step 4: Calculate the new width and height with the desired aspect ratio\n",
    "# Step 4: Calculate the new width and height with the desired aspect ratio\n",
    "desired_height = 1080\n",
    "aspect_ratio = width / height\n",
    "new_width = int(desired_height * aspect_ratio)\n",
    "\n",
    "# Step 5: Create VideoWriter object to write the resized video\n",
    "output_filename = \"resized_tofu.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(output_filename, fourcc, fps, (new_width, desired_height))\n",
    "\n",
    "# Step 6: Read frames from the original video, resize, and write to the new video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (new_width, desired_height))\n",
    "    out.write(resized_frame)\n",
    "\n",
    "# Step 7: Release VideoCapture and VideoWriter objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Print the height and width of the resized video\n",
    "print(\"Height:\", desired_height)\n",
    "print(\"Width:\", new_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "467957d3-5389-4fcb-a04f-6d8fc55f6da2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     resized_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (new_width, desired_height))\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mresized_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(resized_frame)\n\u001b[0;32m     34\u001b[0m video \u001b[38;5;241m=\u001b[39m ImageSequenceClip(resized_frame,fps\u001b[38;5;241m=\u001b[39mcap\u001b[38;5;241m.\u001b[39mfps)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Step 7: Release VideoCapture and VideoWriter objects\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Step 1: Set the video filename\n",
    "video_filename = \"tofu.mp4\"\n",
    "\n",
    "# Step 2: Create a VideoCapture object from the video\n",
    "cap = cv2.VideoCapture(video_filename)\n",
    "\n",
    "# Step 3: Get the height and width of the video\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# Step 4: Calculate the new width and height with the desired aspect ratio\n",
    "desired_height = 1080\n",
    "aspect_ratio = width / height\n",
    "new_width = int(desired_height * aspect_ratio)\n",
    "\n",
    "# Step 5: Create VideoWriter object to write the resized video\n",
    "output_filename = \"resized_tofu.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter(output_filename, fourcc, fps, (new_width, desired_height))\n",
    "\n",
    "# Step 6: Read frames from the original video, resize, and write to the new video\n",
    "resized_frame = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    resized_frame = cv2.resize(frame, (new_width, desired_height))\n",
    "    resized_frame.append(resized_frame)\n",
    "\n",
    "video = ImageSequenceClip(resized_frame,fps=cap.fps)\n",
    "# Step 7: Release VideoCapture and VideoWriter objects\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Print the height and width of the resized video\n",
    "print(\"Height:\", desired_height)\n",
    "print(\"Width:\", new_width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97e4bf-4864-4660-b278-d3b3b7e9fe70",
   "metadata": {},
   "source": [
    "# Moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f90a05-2078-4169-9f9b-4dfb8489bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 1080\n",
      "Width: 1920\n",
      "Moviepy - Building video concatenated_video.mp4.\n",
      "Moviepy - Writing video concatenated_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready concatenated_video.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "import cv2\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\", \"feel.mp4\"]\n",
    "\n",
    "# Step 2: Create a list to store the ImageSequenceClip objects for each video\n",
    "video_clips = []\n",
    "\n",
    "for filename in video_filenames:\n",
    "    # Step 3: Create a VideoCapture object from the video\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "\n",
    "    # Step 4: Get the height and width of the video\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    # Step 5: Calculate the new width and height with the desired aspect ratio\n",
    "    desired_height = 1080\n",
    "    aspect_ratio = width / height\n",
    "    new_width = int(desired_height * aspect_ratio)\n",
    "\n",
    "    # Step 6: Read frames from the original video, resize, and store in a list\n",
    "    resized_frames = []\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (new_width, desired_height))\n",
    "\n",
    "        # Append the resized_frame to the list\n",
    "        resized_frames.append(resized_frame)\n",
    "\n",
    "    # Step 7: Release VideoCapture object\n",
    "    cap.release()\n",
    "\n",
    "    # Step 8: Create an ImageSequenceClip from the list of resized frames and add it to the video_clips list\n",
    "    video_clips.append(ImageSequenceClip(resized_frames, fps=fps))\n",
    "\n",
    "# Print the height and width of the resized videos\n",
    "print(\"Height:\", desired_height)\n",
    "print(\"Width:\", new_width)\n",
    "\n",
    "# Step 9: Now you have a list of ImageSequenceClip objects for each video\n",
    "# You can use these video_clips for further processing or editing with moviepy\n",
    "# For example, you can concatenate them to create a single video or apply other transformations.\n",
    "\n",
    "# For example, to concatenate the clips and write the final video:\n",
    "final_clip = concatenate_videoclips(video_clips, method=\"compose\")\n",
    "final_clip.write_videofile(\"concatenated_video.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9416821a-4133-48f2-8dfb-02dc9563add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 1080.0\n",
      "Width: 607.0\n",
      "Aspect Ratio: 0.56:1.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"sweet-foods-causing-diabetes_duration_9_second.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the width and height of the video\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(\"Height:\", height)\n",
    "print(\"Width:\", width)\n",
    "# Calculate the aspect ratio\n",
    "aspect_ratio = width / height\n",
    "\n",
    "print(\"Aspect Ratio: {:.2f}:{:.2f}\".format(aspect_ratio, 1))\n",
    "\n",
    "# Release the video capture\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57d699b-8d63-4872-96d8-228d6e4c3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'video_clips[0]+slidein()+video_clips[1]'\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, None)], method='compose')\n",
    "\n",
    "string = 'video_clips[1]+slidein()+video_clips[2]'\n",
    "final_clip = concatenate_videoclips([video_clips[1].subclip(0, video_clips[1].duration - 1), slidein(video_clips[1],video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')\n",
    "\n",
    "string = 'video_clips[0]+slidein()+video_clips[1]+fadedown()+video_clips[2]'\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1),fadedown(video_clips[1],video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')\n",
    "\n",
    "string = 'video_clips[0]+slidein()+video_clips[1]+fadedown()+video_clips[2]+fadein()+video_clips[3]'\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0],video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1),fadedown(video_clips[1],video_clips[2]), video_clips[2].subclip(1.3, video_clips[2].duration - 1),fadein(video_clips[2],video_clips[3]),video_clips[3].subclip(1.3, None))], method='compose')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24dc8b00-4af1-4eda-872f-b53f704ec373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), fuckdein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, None), ], method='compose')\n",
      "concatenate_videoclips([video_clips[1].subclip(0, video_clips[1].duration - 1), slidein(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, None), ], method='compose')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_string_to_final_clip(input_string):\n",
    "    # Define regular expression patterns to match video_clips[i] and custom function expressions\n",
    "    video_clip_pattern = r'video_clips\\[(\\d+)\\]'\n",
    "    custom_function_pattern = r'(\\w+)\\(\\)'\n",
    "\n",
    "    # Find all occurrences of video_clips[i] and custom function expressions in the input string\n",
    "    video_clip_matches = re.findall(video_clip_pattern, input_string)\n",
    "    custom_function_matches = re.findall(custom_function_pattern, input_string)\n",
    "\n",
    "    # Check if the input string contains at least one video_clips[i] and one custom function expression\n",
    "    if not video_clip_matches or not custom_function_matches:\n",
    "        return \"Invalid input string. It should contain at least one video_clips[i] and one custom function expression.\"\n",
    "\n",
    "    # Construct the final_clip string\n",
    "    final_clip_str = \"concatenate_videoclips([\"\n",
    "\n",
    "    num_clips = min(len(video_clip_matches), len(custom_function_matches))\n",
    "    for i in range(num_clips):\n",
    "        video_idx = int(video_clip_matches[i])\n",
    "        custom_function_name = custom_function_matches[i]\n",
    "\n",
    "        custom_function_str = \"{}(video_clips[{}], video_clips[{}]), \".format(custom_function_name, video_idx, video_idx + 1)\n",
    "\n",
    "        subclip_str1 = \"video_clips[{}].subclip(0, video_clips[{}].duration - 1), \".format(video_idx, video_idx)\n",
    "        subclip_str2 = \"video_clips[{}].subclip(1.3, None), \".format(video_idx + 1)\n",
    "\n",
    "        final_clip_str += \"{}{}{}, \".format(subclip_str1, custom_function_str, subclip_str2)\n",
    "\n",
    "    # Remove the last two characters \", \" from the final_clip_str\n",
    "    final_clip_str = final_clip_str[:-2]\n",
    "\n",
    "    # Add the method argument and closing bracket\n",
    "    final_clip_str += \"], method='compose')\"\n",
    "\n",
    "    return final_clip_str\n",
    "\n",
    "# Test examples\n",
    "input_string1 = 'video_clips[0]+slidein()+video_clips[1]'\n",
    "input_string2 = 'video_clips[1]+slidein()+video_clips[2]'\n",
    "\n",
    "print(convert_string_to_final_clip(input_string1))\n",
    "print(convert_string_to_final_clip(input_string2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6359b8ec-e1e5-40dd-8c5c-9fdd3f3caa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1), video_clips[1].subclip(0, video_clips[1].duration - 1), slidein(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, None)], method='compose')\n",
      "concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), slidein(video_clips[0], video_clips[1]), video_clips[1].subclip(1.3, video_clips[1].duration - 1), video_clips[1].subclip(0, video_clips[1].duration - 1), fadedown(video_clips[1], video_clips[2]), video_clips[2].subclip(1.3, video_clips[2].duration - 1), video_clips[2].subclip(0, video_clips[2].duration - 1), fadein(video_clips[2], video_clips[3]), video_clips[3].subclip(1.3, video_clips[3].duration - 1), video_clips[3].subclip(0, video_clips[3].duration - 1), fun(video_clips[3], video_clips[4]), video_clips[4].subclip(1.3, video_clips[4].duration - 1), video_clips[4].subclip(0, video_clips[4].duration - 1), funck(video_clips[4], video_clips[5]), video_clips[5].subclip(1.3, None)], method='compose')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_string_to_final_clip(input_string):\n",
    "    # Define regular expression patterns to match video_clips[i] and custom function expressions\n",
    "    video_clip_pattern = r'video_clips\\[(\\d+)\\]'\n",
    "    custom_function_pattern = r'(\\w+)\\(\\)'\n",
    "\n",
    "    # Find all occurrences of video_clips[i] and custom function expressions in the input string\n",
    "    video_clip_matches = re.findall(video_clip_pattern, input_string)\n",
    "    custom_function_matches = re.findall(custom_function_pattern, input_string)\n",
    "\n",
    "    # Check if the input string contains at least one video_clips[i] and one custom function expression\n",
    "    if not video_clip_matches or not custom_function_matches:\n",
    "        return \"Invalid input string. It should contain at least one video_clips[i] and one custom function expression.\"\n",
    "\n",
    "    # Construct the final_clip string\n",
    "    final_clip_str = \"concatenate_videoclips([\"\n",
    "\n",
    "    num_clips = min(len(video_clip_matches), len(custom_function_matches))\n",
    "    for i in range(num_clips):\n",
    "        video_idx = int(video_clip_matches[i])\n",
    "        custom_function_name = custom_function_matches[i]\n",
    "\n",
    "        custom_function_str = \"{}(video_clips[{}], video_clips[{}])\".format(custom_function_name, video_idx, video_idx + 1)\n",
    "\n",
    "        subclip_str1 = \"video_clips[{}].subclip(0, video_clips[{}].duration - 1)\".format(video_idx, video_idx)\n",
    "\n",
    "        if i == num_clips - 1:  # Last video clip\n",
    "            subclip_str2 = \"video_clips[{}].subclip(1.3, None)\".format(video_idx + 1)\n",
    "        else:\n",
    "            subclip_str2 = \"video_clips[{}].subclip(1.3, video_clips[{}].duration - 1)\".format(video_idx + 1, video_idx + 1)\n",
    "\n",
    "        final_clip_str += \"{}, {}, {}, \".format(subclip_str1, custom_function_str, subclip_str2)\n",
    "\n",
    "    # Remove the last two characters \", \" from the final_clip_str\n",
    "    final_clip_str = final_clip_str[:-2]\n",
    "\n",
    "    # Add the method argument and closing bracket\n",
    "    final_clip_str += \"], method='compose')\"\n",
    "\n",
    "    return final_clip_str\n",
    "\n",
    "# Test examples\n",
    "input_string1 = 'video_clips[0]+slidein()+video_clips[1]+slidein()+video_clips[2]'\n",
    "input_string2 = 'video_clips[0]+slidein()+video_clips[1]+fadedown()+video_clips[2]+fadein()+video_clips[3]+fun()+video_clips[4]+funck()+video_clips[5]'\n",
    "\n",
    "print(convert_string_to_final_clip(input_string1))\n",
    "print(convert_string_to_final_clip(input_string2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8440f3ba-cd15-4503-894b-7bf1cc1e81ee",
   "metadata": {},
   "source": [
    "# not function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e85849-4e9f-407a-b772-9655694041ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video lastslidein.mp4.\n",
      "MoviePy - Writing audio in lastslideinTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video lastslidein.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready lastslidein.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 1  # Duration of the slide-in transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-1, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 1.3)\n",
    "\n",
    "# Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "if clip1_last_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip1_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "    # clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "else:\n",
    "    clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "\n",
    "# Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "if clip2_first_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip2_frames = [cv2.copyMakeBorder(frame, 0, 0, padding, padding, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "    # clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "else:\n",
    "    \n",
    "    clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-in transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / num_frames  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    tx_clip1 = -int((width / num_frames) * i)\n",
    "    tx_clip2 = width - int((width / num_frames) * i)\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, tx_clip1], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, tx_clip2], [0, 1, 0]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Add remaining frames where clip2_first_second fills the entire screen\n",
    "remaining_frames = clip2_frames[num_frames:]\n",
    "transition_frames.extend(remaining_frames)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), transition_clip.set_duration(transition_duration), video_clips[1].subclip(1.3, None)], method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"lastslidein.mp4\", codec=\"libx264\", fps=final_clip.fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf162210-10ba-4f28-8f2d-b55b1f6c84b8",
   "metadata": {},
   "source": [
    "# Down to UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4830b90e-9a8f-4005-87d1-ca16f368d8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video slidedowntoup.mp4.\n",
      "MoviePy - Writing audio in slidedowntoupTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video slidedowntoup.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready slidedowntoup.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "\n",
    "# Step 1: Set the video filenames\n",
    "video_filenames = [\"fancy.mp4\", \"motion.mp4\"]\n",
    "\n",
    "# Step 2: Create VideoFileClip objects from the videos\n",
    "video_clips = []\n",
    "for filename in video_filenames:\n",
    "    video = VideoFileClip(filename)\n",
    "    video = video.resize(height=1080)\n",
    "    video = video.set_position((\"center\", \"center\"))  # Set video position to center\n",
    "    \n",
    "    video_clips.append(video)\n",
    "\n",
    "# Step 3: Define transition duration\n",
    "transition_duration = 1  # Duration of the slide-down-to-up transition in seconds\n",
    "\n",
    "# Step 4: Extract the last one second of clip1 and the first one second of clip2\n",
    "clip1_last_second = video_clips[0].subclip(-1, None)\n",
    "clip2_first_second = video_clips[1].subclip(0, 1)\n",
    "\n",
    "# Check if aspect ratio of clip1_last_second is less than 1.78:1 and add black padding\n",
    "if clip1_last_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip1_last_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip1_frames = [cv2.copyMakeBorder(frame, padding, padding, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip1_last_second.iter_frames()]\n",
    "else:\n",
    "    clip1_frames = [frame for frame in clip1_last_second.iter_frames()]\n",
    "\n",
    "# Check if aspect ratio of clip2_first_second is less than 1.78:1 and add black padding\n",
    "if clip2_first_second.aspect_ratio < 1.78:\n",
    "    width = int(1080 * clip2_first_second.aspect_ratio)\n",
    "    padding = (1920 - width) // 2\n",
    "    clip2_frames = [cv2.copyMakeBorder(frame, padding, padding, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0]) for frame in clip2_first_second.iter_frames()]\n",
    "else:\n",
    "    clip2_frames = [frame for frame in clip2_first_second.iter_frames()]\n",
    "\n",
    "# Step 6: Perform slide-down-to-up transition using numpy and cv2\n",
    "transition_frames = []\n",
    "height, width, _ = clip1_frames[0].shape\n",
    "\n",
    "# Ensure the number of frames matches the expected duration\n",
    "num_frames = min(len(clip1_frames), len(clip2_frames), int(transition_duration * clip1_last_second.fps))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    alpha = i / num_frames  # Linearly increasing alpha value\n",
    "\n",
    "    # Calculate the translation distance based on the frame index\n",
    "    ty_clip1 = int((height / num_frames) * i)\n",
    "    ty_clip2 = height - int((height / num_frames) * i)\n",
    "\n",
    "    # Apply translation to clip1\n",
    "    frame_clip1 = cv2.warpAffine(clip1_frames[i], np.float32([[1, 0, 0], [0, 1, ty_clip1]]), (width, height))\n",
    "\n",
    "    # Handle the case where clip2_frames has fewer frames than expected\n",
    "    if i < len(clip2_frames):\n",
    "        # Apply translation to clip2\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[i], np.float32([[1, 0, 0], [0, 1, ty_clip2]]), (width, height))\n",
    "    else:\n",
    "        # Use the last frame of clip2_frames for the remaining frames\n",
    "        frame_clip2 = cv2.warpAffine(clip2_frames[-1], np.float32([[1, 0, 0], [0, 1, ty_clip2]]), (width, height))\n",
    "\n",
    "    # Blend the frames to create the transition effect\n",
    "    blend_frame = cv2.addWeighted(frame_clip1, 1 - alpha, frame_clip2, alpha, 0)\n",
    "\n",
    "    transition_frames.append(blend_frame)\n",
    "\n",
    "# Add remaining frames where clip2_first_second fills the entire screen\n",
    "remaining_frames = clip2_frames[num_frames:]\n",
    "transition_frames.extend(remaining_frames)\n",
    "\n",
    "# Step 7: Concatenate the original clips and the transition frames to create the final video\n",
    "transition_clip = ImageSequenceClip(transition_frames, fps=clip1_last_second.fps)\n",
    "final_clip = concatenate_videoclips([video_clips[0].subclip(0, video_clips[0].duration - 1), transition_clip.set_duration(transition_duration), video_clips[1].subclip(1, None)], method='compose')\n",
    "\n",
    "# Step 8: Write the final video to a file\n",
    "final_clip.write_videofile(\"slidedowntoup.mp4\", codec=\"libx264\", fps=final_clip.fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b355e3eb-b6c3-4149-ab64-9f49d3751997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video 3_final_clip.mp4.\n",
      "Moviepy - Writing video 3_final_clip.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready 3_final_clip.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageClip, CompositeVideoClip\n",
    "from moviepy.editor import transfx\n",
    "\n",
    "EFFECT_DURATION = 0.3\n",
    "CLIP_DURATION = 7\n",
    "\n",
    "clip1 = ImageClip(\"m.jpeg\").set_duration(CLIP_DURATION)\n",
    "clip2 = ImageClip(\"n.jpeg\").set_duration(CLIP_DURATION)\n",
    "clip3 = ImageClip(\"nn.jpg\").set_duration(CLIP_DURATION)\n",
    "\n",
    "clips = [clip1, clip2, clip3]\n",
    "num_clips = len(clips)\n",
    "\n",
    "# Calculate the target width and height while maintaining aspect ratio\n",
    "target_width = 1920\n",
    "target_height = int(target_width * clip1.size[1] / clip1.size[0])\n",
    "\n",
    "# Create a list to store the final clips\n",
    "final_clips = []\n",
    "\n",
    "for i, clip in enumerate(clips):\n",
    "    start_time = (CLIP_DURATION - EFFECT_DURATION) * i\n",
    "\n",
    "    # Apply slide out effect to the current clip\n",
    "    slide_out_clip = clip.set_position(\"center\").fx(transfx.slide_out, duration=EFFECT_DURATION, side=\"left\")\n",
    "    slide_out_clip = slide_out_clip.set_start(start_time)\n",
    "    slide_out_clip = slide_out_clip.resize(width=target_width, height=target_height)\n",
    "\n",
    "    final_clips.append(slide_out_clip)\n",
    "\n",
    "    if i < num_clips - 1:\n",
    "        # Apply slide in effect to the next clip\n",
    "        slide_in_clip = clips[i+1].set_position(\"center\").fx(transfx.slide_in, duration=EFFECT_DURATION, side=\"right\")\n",
    "        slide_in_clip = slide_in_clip.set_start(start_time + EFFECT_DURATION)\n",
    "        slide_in_clip = slide_in_clip.resize(width=target_width, height=target_height)\n",
    "\n",
    "        final_clips.append(slide_in_clip)\n",
    "\n",
    "# Concatenate the final clips into a single video clip\n",
    "final_video_clip = CompositeVideoClip(final_clips, size=(target_width, target_height))\n",
    "\n",
    "# Write the video file\n",
    "final_video_clip.write_videofile(\"3_final_clip.mp4\", fps=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9eb65b5-2640-45c8-8444-9c36a26c5160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 720\n",
      "Width: 406\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Import the video\n",
    "cap = cv2.VideoCapture(\"rose.mp4\")\n",
    "\n",
    "# Get the height and width of the video\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "# Print the height and width of the video\n",
    "print(\"Height:\", height)\n",
    "print(\"Width:\", width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6913133c-4ffb-491f-9149-f0dce5f9e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height: 720.0\n",
      "Width: 1280.0\n",
      "Aspect Ratio: 1.78:1.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"fancy.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the width and height of the video\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(\"Height:\", height)\n",
    "print(\"Width:\", width)\n",
    "# Calculate the aspect ratio\n",
    "aspect_ratio = width / height\n",
    "\n",
    "print(\"Aspect Ratio: {:.2f}:{:.2f}\".format(aspect_ratio, 1))\n",
    "\n",
    "# Release the video capture\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d16a15-d3eb-48cf-8e03-1de801dc704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "\n",
    "# Load the video clips\n",
    "clip1 = VideoFileClip('fancy.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "clip2 = VideoFileClip('motion.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "\n",
    "# Resize the clips to match the aspect ratio of the first clip (fancy.mp4)\n",
    "clip2 = clip2.resize(height=clip1.size[1])\n",
    "\n",
    "# Concatenate the clips\n",
    "combined = concatenate_videoclips([clip1, clip2])\n",
    "\n",
    "# Write the final video file\n",
    "combined.write_videofile('new.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99352208-8f25-441c-957e-987799f856b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "\n",
    "# Set the desired aspect ratio\n",
    "desired_aspect_ratio = 16 / 9  # For example, 16:9\n",
    "\n",
    "# Load the video clips\n",
    "clip1 = VideoFileClip('fancy.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "clip2 = VideoFileClip('1686498673.mp4').subclip(0, 6).fx(vfx.fadein, 1).fx(vfx.fadeout, 1)\n",
    "\n",
    "# Calculate the current aspect ratios of the clips\n",
    "aspect_ratio1 = clip1.size[0] / clip1.size[1]\n",
    "aspect_ratio2 = clip2.size[0] / clip2.size[1]\n",
    "\n",
    "# Resize the clips to match the desired aspect ratio\n",
    "clip1_resized = clip1.resize(height=int(clip1.size[0] / desired_aspect_ratio))\n",
    "clip2_resized = clip2.resize(height=int(clip2.size[0] / desired_aspect_ratio))\n",
    "\n",
    "# Concatenate the resized clips\n",
    "final_clip = concatenate_videoclips([clip1_resized, clip2_resized])\n",
    "\n",
    "# Write the final video file\n",
    "final_clip.write_videofile('new.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49b814-443b-4ddb-b4e9-4b4e0b67f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def export_black_video(filename, width, height, duration):\n",
    "  \"\"\"Exports a full black video to the specified filename.\n",
    "\n",
    "  Args:\n",
    "    filename: The filename of the video to export.\n",
    "    width: The width of the video.\n",
    "    height: The height of the video.\n",
    "    duration: The duration of the video in seconds.\n",
    "  \"\"\"\n",
    "\n",
    "  video = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))\n",
    "  black_frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "  for i in range(duration * 30):\n",
    "    video.write(black_frame)\n",
    "  video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  export_black_video('black.mp4', 1280, 720, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0d55e-cdeb-4333-bda9-7512e35a020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.video.fx.resize import resize\n",
    "\n",
    "# Load the video clip\n",
    "clip = VideoFileClip(\"motion.mp4\").subclip(0, 6)\n",
    "\n",
    "# Calculate the desired height based on the 16:9 aspect ratio\n",
    "desired_height = int(clip.w * 9 / 16)\n",
    "\n",
    "# Resize the clip while maintaining the aspect ratio\n",
    "resized_clip = resize(clip, height=desired_height)\n",
    "\n",
    "# Write the resized clip to a new file\n",
    "resized_clip.write_videofile(\"motion_resized.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7dfa4-fe09-41fd-b110-c75be9d1c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "from moviepy.video.fx.resize import resize\n",
    "\n",
    "# Load the video clips\n",
    "clip_fancy = VideoFileClip(\"fancy.mp4\").subclip(0, 6)\n",
    "# Load the video clip\n",
    "clip = VideoFileClip(\"motion.mp4\").subclip(0, 6)\n",
    "\n",
    "# Calculate the desired height based on the 16:9 aspect ratio\n",
    "desired_height = int(clip.w * 9 / 16)\n",
    "\n",
    "# Resize the clip while maintaining the aspect ratio\n",
    "resized_clip = resize(clip, height=desired_height)\n",
    "# Concatenate the clips\n",
    "combined_clip = concatenate_videoclips([clip_fancy, resized_clip])\n",
    "\n",
    "# Write the combined clip to a new file\n",
    "combined_clip.write_videofile(\"combined.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b9bb6-7490-4f2f-99f6-9e12a4df3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "clip = VideoFileClip(\"motion.mp4\").subclip(0,6)\n",
    "\n",
    "combine = clips_array([[clip]])\n",
    "cambiado = combine.fx(vfx.resize,(1920,1080),width= 1980)\n",
    "cambiado.write_videofile(\"combina5v.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fcaef-7639-4f87-8d7a-5eb7221cfde7",
   "metadata": {},
   "source": [
    "# PERFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59696cd5-94fd-4356-aeec-42d4a57dbe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "600\n",
      "Moviepy - Building video motion_on_black_background.mp4.\n",
      "MoviePy - Writing audio in motion_on_black_backgroundTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video motion_on_black_background.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready motion_on_black_background.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "# Load the motion clip\n",
    "clip_motion = VideoFileClip(r\"fancy.mp4\").subclip(0, 6)\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1920\n",
    "\n",
    "# Create a black color clip as the background\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(6)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = (background_width - clip_motion.w) // 2\n",
    "y_pos = (background_height - clip_motion.h) // 2\n",
    "print(x_pos)\n",
    "print(y_pos)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "final_clip.write_videofile(\"motion_on_black_background.mp4\", fps=clip_motion.fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625d00e1-617f-4173-b08c-164a96bdb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "600\n",
      "Moviepy - Building video last.mp4.\n",
      "MoviePy - Writing audio in lastTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video last.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready last.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "# Load the motion clip\n",
    "clip_motion = VideoFileClip(r\"fancy.mp4\").subclip(0, 6)\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1920\n",
    "\n",
    "# Create a black color clip as the background\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(6)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = (background_width - clip_motion.w) // 2\n",
    "y_pos = (background_height - clip_motion.h) // 2\n",
    "print(x_pos)\n",
    "print(y_pos)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combine = final_clip.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "combine.write_videofile(\"last.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a78d0af1-a2a7-4f36-85b4-653393f6b4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video my_reels.mp4.\n",
      "MoviePy - Writing audio in my_reelsTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video my_reels.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready my_reels.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1980\n",
    "\n",
    "# Create a black color clip as the background\n",
    "\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(10)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = 0\n",
    "y_pos = 1\n",
    "clip_motion = VideoFileClip(r\"motion.mp4\").subclip(0, 6)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "final_clip.write_videofile(\"my_reels.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d91edc14-b1eb-46c5-bfa7-5329a728c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video my_reels.mp4.\n",
      "MoviePy - Writing audio in my_reelsTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video my_reels.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready my_reels.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, ColorClip, CompositeVideoClip\n",
    "\n",
    "\n",
    "# Define the width and height of the black background\n",
    "background_width = 1080\n",
    "background_height = 1980\n",
    "\n",
    "# Create a black color clip as the background\n",
    "\n",
    "background_clip = ColorClip((background_width, background_height), color=(0, 0, 0)).set_duration(10)\n",
    "\n",
    "# Calculate the position to center the motion clip on the background\n",
    "x_pos = 0\n",
    "y_pos = 1\n",
    "clip_motion = VideoFileClip(r\"fancy.mp4\").subclip(0, 6)\n",
    "# Composite the motion clip onto the black background\n",
    "final_clip = CompositeVideoClip([background_clip.set_position((x_pos, y_pos)), clip_motion.set_position((x_pos, y_pos))])\n",
    "\n",
    "# Write the final clip to a new file\n",
    "final_clip.write_videofile(\"my_reels.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57a855-a6a4-4b50-80c8-3c929b1e7052",
   "metadata": {},
   "source": [
    "# Multiple screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c71d6-c82c-4b77-a669-e279d65b5f62",
   "metadata": {},
   "source": [
    "# Don't use 4k, it's so slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0327c97-1110-4e31-a438-4f566c9770cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video combined1.mp4.\n",
      "MoviePy - Writing audio in combined1TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video combined1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready combined1.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "clip1 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp4\").subclip(87, 93).margin(8)\n",
    "clip2 = VideoFileClip(\"fancy.mp4\").subclip(0, 6).margin(8)\n",
    "clip3 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp4\").subclip(87, 93).margin(8)\n",
    "clip4 = VideoFileClip(\"fancy.mp4\").subclip(0, 6).margin(8)\n",
    "clip2.resize((1920,1080))\n",
    "clip4.resize((1920,1080))\n",
    "combine = clips_array([[clip1, clip2],\n",
    "                       [clip3, clip4]])\n",
    "\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combine = combine.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "\n",
    "# Write the combined clips to a new file\n",
    "combine.write_videofile('combined1.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d3be3-b5c8-4811-a4d1-b94fb01c394b",
   "metadata": {},
   "source": [
    "# Perfect export of different resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1ec1fb-ee1b-4cd8-8cf1-f41d30bb7564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video dahyun0.mp4.\n",
      "MoviePy - Writing audio in dahyun0TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video dahyun0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready dahyun0.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, vfx\n",
    "from moviepy.editor import *\n",
    "# clip1 = VideoFileClip(r\"D:\\video\\Video\\Video\\twice\\twice.mp\").subclip(87, 93)\n",
    "clip1 = VideoFileClip(\"feel.mp4\",target_resolution=(1080, 1920)).subclip(0,6)\n",
    "clip2 = VideoFileClip(\"dubu.mp4\").subclip(0,6)\n",
    "\n",
    "combine = concatenate_videoclips([clip1,clip2],method=\"compose\")\n",
    "# Set the target resolution to HD (1920x1080)\n",
    "target_resolution = (1920, 1080)\n",
    "\n",
    "# Resize the combined clip to the target resolution\n",
    "combined = combine.resize(height=target_resolution[1], width=target_resolution[0])\n",
    "combined.write_videofile('dahyun0.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5dc9a7-e7b2-4721-96ed-bb47e064f365",
   "metadata": {},
   "source": [
    "# traget resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499b93ba-5e24-429d-a806-a155c5ac5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"combined.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define the target resolution\n",
    "target_width = 1080\n",
    "target_height = 1920\n",
    "\n",
    "# Get the original video's width and height\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "# Calculate the aspect ratio of the original video\n",
    "aspect_ratio = width / height\n",
    "\n",
    "# Calculate the new dimensions for resizing\n",
    "if aspect_ratio > target_width / target_height:\n",
    "    new_width = target_width\n",
    "    new_height = int(target_width / aspect_ratio)\n",
    "else:\n",
    "    new_height = target_height\n",
    "    new_width = int(target_height * aspect_ratio)\n",
    "\n",
    "# Create an output video writer\n",
    "output_path = \"output_combined_hd.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))\n",
    "\n",
    "# Process each frame of the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to the target dimensions\n",
    "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    # Write the resized frame to the output video\n",
    "    out.write(resized_frame)\n",
    "\n",
    "    # Display the resized frame\n",
    "    cv2.imshow(\"Resized Frame\", resized_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and output video writer\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d0e8069-8905-4d02-8320-ee080616cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = \"test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        # End of the video\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Reels Video\", frame)\n",
    "\n",
    "    # Check for the 'q' key to quit\n",
    "    if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af55bb49-c522-4268-aca2-91ad9637ee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of active threads: 7\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "# Get the number of active threads\n",
    "num_threads = threading.active_count()\n",
    "\n",
    "print(\"Number of active threads:\", num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e5e1a-7482-439b-b7b7-10562f972965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
